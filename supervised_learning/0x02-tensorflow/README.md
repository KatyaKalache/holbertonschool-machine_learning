![alt text](https://holbertonintranet.s3.amazonaws.com/uploads/medias/2018/11/2bc924532bc4a901e74d.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIARDDGGGOUXW7JF5MT%2F20190724%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20190724T005613Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=ebca7583678b3adf5bf8f4307b22fb56d5f367d9a50e108f3fc325e4820003bf)
## Description
0x03. Optimization
## Specialization
Machine Learning

| Learning Objectives  |
| ---------------- |
|    `What is a hyperparameter?`   |
|    `How and why do you normalize your input data?`   |
|    `What is a saddle point?`   |
|    `What is stochastic gradient descent?`   |
|    `What is mini-batch gradient descent?`   |
|    `What is a moving average? How do you implement it?`   |
|    `What is gradient descent with momentum? How do you implement it?`   |
|    `What is RMSProp? How do you implement it?`   |
|    `What is Adam optimization? How do you implement it?`   |
|    `What is learning rate decay? How do you implement it?`   |
|    `What is batch normalization? How do you implement it?`   |

Ekaterina Kalache: [github account](https://github.com/KatyaKalache), [twitter](https://twitter.com/KatyaKalache)
